import sys
import get_data
import KNN
import Naive_Bayes_pyV2 as Naive_Bayes
import DecisionTree
from sklearn.model_selection import train_test_split, ShuffleSplit
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn import tree
from sklearn.metrics import f1_score
import numpy as np
import pandas as pd
import itertools
import matplotlib.pyplot as plt
import warnings

def visual_compare_methods():
	max_features = 500
	datasets = np.array(['32474', '19804', '27562', '59856', '33315'])
	f_measure_each = np.array([[0] * int(max_features/100)] * (len(datasets)))
	counter = 0
	method_combo = []
	
	feature_selection = ['Original Features',
	'Semi-supervised variance by mean ratio',
	'Unsupervised variance by mean ratio', 
	'Chi Square', 'ANOVA F-classifier']
	classifiers = ['KNN', 'Naive Bayes', 'Decision Trees']

	for model in classifiers:
		for feature_method in feature_selection:
			for dataset in datasets:
			filename = '/Users/GodSpeed/Documents/Courses/Bioinformatics/Project/Datasets/GSE' + dataset + '_series_matrix.txt'
			X, Y, df, df_type, class_names = get_data.get_data(filename)
			Xtemp = X
			for num_of_dims in range(100, max_features + 1, 100):
				fmeasure_expe = []
				for iter in range(0,1):
					X = Xtemp
					if feature_method == 'Supervised variance by mean ratio':
						feature_importance_order = KNN.feature_select_var(X, Y, 
							df_type, class_names)
						X = X[:, np.array(feature_importance_order[
							0:num_of_dims])]
					elif feature_method == 'Semi-supervised variance by mean ratio':
						feature_importance_order = KNN.feature_std_distance(X, Y, 
							df_type, class_names)
						X = X[:, np.array(feature_importance_order[
							0:num_of_dims])]
					elif feature_method == 'Unsupervised variance by mean ratio':
						feature_importance_order = KNN.feature_select_std(X)
						X = X[:, np.array(feature_importance_order[
							0:num_of_dims])]
					elif feature_method == 'Chi Square':
						X = Naive_Bayes.feature_reduce(X, Y, num_of_dims)
					elif feature_method == 'ANOVA F-classifier':
						X = DecisionTree.feature_reduce_f_class_if(X, Y, num_of_dims)

					#splitting data into training and testing set
					X_train, X_test, Y_train, Y_test = train_test_split(X, Y, 
						test_size = 0.2)

					if model == 'KNN':
						clf = KNN.train_model(KNeighborsClassifier( 
							n_neighbors = 1), X_train, Y_train)
					elif model == 'Naive Bayes':
						clf = Naive_Bayes.train_model(GaussianNB(), X_train, 
							Y_train)
					elif model == 'Decision Trees':
						clf = DecisionTree.train_model(
							tree.DecisionTreeClassifier(), X_train, Y_train)

					Y_pred = clf.predict(X_test)

					#checking accuracy
					fmeasure = np.around(f1_score(Y_test, Y_pred, average = 
						'weighted'), decimals = 2)
					fmeasure_expe.append(fmeasure)

				fmeasure = np.mean(fmeasure_expe) * 100
				#row = feature_selection.index(feature_method)
				#col = classifiers.index(model)
				f_measure_each[counter][int(num_of_dims/100) - 1] = fmeasure
			counter = counter + 1

	print f_measure_each
	f_measure_dims = pd.DataFrame(data = f_measure_each, 
		columns = np.arange(100, max_features + 1, 100),
		index = datasets)
	print f_measure_dims
	return performance, feature_selection, classifiers, f_measure_dims

def main():
	warnings.filterwarnings("ignore")
